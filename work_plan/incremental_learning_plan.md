# ì ì§„ì  í•™ìŠµ(Incremental Learning) êµ¬í˜„ ê¸°íšì„œ

> **ì‘ì„±ì¼**: 2025-12-25  
> **ëª©í‘œ**: ì €ì¥ëœ AI ëª¨ë¸ì— ìƒˆë¡œìš´ ë°ì´í„°ë§Œ ì¶”ê°€ í•™ìŠµí•˜ì—¬ íš¨ìœ¨ì„± í–¥ìƒ

---

## 1. ë¬¸ì œ ì •ì˜

### í˜„ì¬ ìƒí™©
- **12/21** í•™ìŠµ: ê³¼ê±° 1ë…„ ë°ì´í„°(~12/20) â†’ ëª¨ë¸ ì €ì¥
- **12/25** ì˜ˆì¸¡: ê³¼ê±° 1ë…„ ë°ì´í„°(~12/24) â†’ **ì „ì²´ ì¬í•™ìŠµ** (ë¹„íš¨ìœ¨)

### ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­
- 12/25ì— ì˜ˆì¸¡ ì‹œ, **12/21~12/24 ì‹ ê·œ ë°ì´í„°ë§Œ** í•™ìŠµí•˜ê³  ê¸°ì¡´ ì§€ì‹ ìœ ì§€
- í•™ìŠµ ì‹œê°„ ë‹¨ì¶• ë° ì»´í“¨íŒ… ìì› ì ˆì•½

---

## 2. ê¸°ìˆ ì  ë°°ê²½

### ì ì§„ì  í•™ìŠµ(Incremental Learning)ì´ë€?
ê¸°ì¡´ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ìƒˆë¡œìš´ ë°ì´í„°ë¡œ **ë¶€ë¶„ ì—…ë°ì´íŠ¸**í•˜ëŠ” ê¸°ë²•.

### ì£¼ìš” ë„ì „ ê³¼ì œ: Catastrophic Forgetting
- **ë¬¸ì œ**: ìƒˆ ë°ì´í„° í•™ìŠµ ì‹œ ì´ì „ì— í•™ìŠµí•œ íŒ¨í„´ì„ ìŠì–´ë²„ë¦¼
- **í•´ê²° ë°©ì•ˆ**:
  1. **ë‚®ì€ Learning Rate** (ê¸°ì¡´ 0.001 â†’ 0.0001)
  2. **ì ì€ Epochs** (ê¸°ì¡´ 50 â†’ 3~5 ì—í­)
  3. **Replay Buffer**: ê³¼ê±° ë°ì´í„° ì¼ë¶€(10%)ë¥¼ í•¨ê»˜ í•™ìŠµ
  4. **Early Stopping**: ê²€ì¦ ì†ì‹¤ ëª¨ë‹ˆí„°ë§

---

## 3. êµ¬í˜„ ê³„íš

### Phase A: ë©”íƒ€ë°ì´í„° ë° ì¸í”„ë¼ êµ¬ì¶•

#### Task 1: í•™ìŠµ ë©”íƒ€ë°ì´í„° ì €ì¥
- **íŒŒì¼**: `src/models/predictor.py`
- **ë‚´ìš©**:
  ```python
  # ëª¨ë¸ ì €ì¥ ì‹œ ë©”íƒ€ë°ì´í„° ì¶”ê°€
  metadata = {
      'last_train_date': '2025-12-21',
      'data_end_date': '2025-12-20',
      'total_samples': 252,
      'feature_cols': [...],
      'model_type': 'lstm'
  }
  # metadata.jsonìœ¼ë¡œ ì €ì¥
  ```
- **ì €ì¥ ìœ„ì¹˜**: `saved_models/{ticker}_{date}_metadata.json`

#### Task 2: ì‹ ê·œ ë°ì´í„° ê°ì§€ ë¡œì§
- **íŒŒì¼**: `src/dashboard/app.py`
- **ë¡œì§**:
  ```python
  if use_saved_model:
      metadata = load_metadata(model_path)
      last_date = metadata['data_end_date']
      new_data = df[df['date'] > last_date]
      
      if len(new_data) > 0:
          show_incremental_option = True
  ```

---

### Phase B: ëª¨ë¸ë³„ ì ì§„ì  í•™ìŠµ êµ¬í˜„

#### Task 3: LSTM/Transformer Fine-tuning
- **íŒŒì¼**: `src/models/predictor.py` â†’ `LSTMPredictor.train()`
- **ë³€ê²½ì‚¬í•­**:
  
  **Before**:
  ```python
  def train(self, df, ...):
      X_train, X_test, y_train, y_test = prepare_lstm_data(df)
      self.model = self.build_model(input_shape)  # ìƒˆ ëª¨ë¸ ìƒì„±
      self.model.fit(X_train, y_train, epochs=50, ...)
  ```
  
  **After**:
  ```python
  def train(self, df, incremental=False, new_data_only=None, ...):
      if incremental and self.model is not None:
          # Fine-tuning ëª¨ë“œ
          X_new, y_new = prepare_lstm_data(new_data_only)
          
          # ë‚®ì€ í•™ìŠµë¥  ì„¤ì •
          from tensorflow.keras.optimizers import Adam
          self.model.compile(
              optimizer=Adam(learning_rate=0.0001),  # ê¸°ì¡´ì˜ 1/10
              loss='mse'
          )
          
          # Replay Buffer: ê³¼ê±° ë°ì´í„° 10% ìƒ˜í”Œë§
          X_old_sample = sample_old_data(df, ratio=0.1)
          X_combined = concat([X_old_sample, X_new])
          
          self.model.fit(
              X_combined, y_combined,
              epochs=5,  # ì ì€ ì—í­
              callbacks=[EarlyStopping(patience=3)],
              verbose=1
          )
      else:
          # ê¸°ì¡´ ì „ì²´ í•™ìŠµ ë¡œì§
          ...
  ```

#### Task 4: XGBoost Incremental Training
- **íŒŒì¼**: `src/models/predictor.py` â†’ `XGBoostClassifier.train()`
- **ë³€ê²½ì‚¬í•­**:
  
  ```python
  def train(self, df, incremental=False, new_data_only=None):
      if incremental and hasattr(self, 'model') and self.model is not None:
          import xgboost as xgb
          
          X_new, y_new = prepare_classification_data(new_data_only)
          dtrain_new = xgb.DMatrix(X_new, label=y_new)
          
          # ê¸°ì¡´ ëª¨ë¸ì—ì„œ ì´ì–´ë°›ê¸°
          params = self.model.get_params()
          self.model = xgb.train(
              params,
              dtrain_new,
              num_boost_round=10,  # ì ì€ ë°˜ë³µ
              xgb_model=self.model.get_booster()  # í•µì‹¬: ê¸°ì¡´ ëª¨ë¸ ì „ë‹¬
          )
      else:
          # ê¸°ì¡´ ì „ì²´ í•™ìŠµ
          ...
  ```

---

### Phase C: UI ë° ì‚¬ìš©ì ê²½í—˜

#### Task 5: ëŒ€ì‹œë³´ë“œ UI ìˆ˜ì •
- **íŒŒì¼**: `src/dashboard/app.py` â†’ `display_ai_prediction()`
- **ìœ„ì¹˜**: "ğŸ’¾ ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°" ì²´í¬ë°•ìŠ¤ ì•„ë˜
- **ì¶”ê°€ UI**:
  
  ```python
  if use_saved_model and new_data_available:
      with st.expander("ğŸ”„ **ì ì§„ì  í•™ìŠµ ì˜µì…˜**", expanded=False):
          st.markdown(f"""
          **ê°ì§€ëœ ì‹ ê·œ ë°ì´í„°**: {len(new_data)}ì¼ì¹˜ ({new_data_start} ~ {new_data_end})
          
          ì ì§„ì  í•™ìŠµì€ ê¸°ì¡´ ëª¨ë¸ì— ìƒˆ ë°ì´í„°ë§Œ ì¶”ê°€ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.
          - âš¡ **ì¥ì **: ë¹ ë¥¸ í•™ìŠµ (ì•½ 1/5 ì‹œê°„)
          - âš ï¸ **ì£¼ì˜**: ì‹ ê·œ ë°ì´í„°ê°€ ë§¤ìš° ì ìœ¼ë©´(~5ì¼) íš¨ê³¼ê°€ ì œí•œì ì¼ ìˆ˜ ìˆìŒ
          """)
          
          use_incremental = st.checkbox(
              "ğŸ”„ ì ì§„ì  í•™ìŠµ ì‚¬ìš©",
              value=True,
              help="ì²´í¬ í•´ì œ ì‹œ ì „ì²´ ë°ì´í„°ë¡œ ì²˜ìŒë¶€í„° ì¬í•™ìŠµí•©ë‹ˆë‹¤."
          )
  ```

#### Task 6: í•™ìŠµ ì§„í–‰ ìƒí™© í‘œì‹œ
- **ê°œì„ **: `st.status`ì—ì„œ ì ì§„ì  í•™ìŠµ ëª¨ë“œ í‘œì‹œ
  
  ```python
  if use_incremental:
      status.write(f"ğŸ”„ ì ì§„ì  í•™ìŠµ ì¤‘... (ì‹ ê·œ ë°ì´í„° {len(new_data)}ì¼ì¹˜)")
  else:
      status.write("ğŸ“Š ì „ì²´ ë°ì´í„° ì¬í•™ìŠµ ì¤‘...")
  ```

---

### Phase D: Ensemble í†µí•©

#### Task 7: EnsemblePredictor ìˆ˜ì •
- **íŒŒì¼**: `src/models/ensemble_predictor.py` â†’ `train_models()`
- **íŒŒë¼ë¯¸í„° ì¶”ê°€**:
  
  ```python
  def train_models(
      self,
      df,
      train_lstm=True,
      train_xgboost=True,
      train_transformer=False,
      incremental=False,       # ì‹ ê·œ
      new_data_only=None,      # ì‹ ê·œ
      ...
  ):
      if train_lstm and self.lstm:
          self.lstm.train(
              df,
              incremental=incremental,
              new_data_only=new_data_only,
              ...
          )
      
      if train_xgboost and self.xgboost:
          self.xgboost.train(
              df,
              incremental=incremental,
              new_data_only=new_data_only
          )
  ```

---

## 4. ê²€ì¦ ê³„íš

### ìë™ í…ŒìŠ¤íŠ¸
1. **ìœ ë‹› í…ŒìŠ¤íŠ¸**: `tests/test_incremental_learning.py`
   - ë©”íƒ€ë°ì´í„° ì €ì¥/ë¡œë“œ
   - ì‹ ê·œ ë°ì´í„° ê°ì§€ ë¡œì§
   - Fine-tuning í˜¸ì¶œ ì—¬ë¶€

### ìˆ˜ë™ ê²€ì¦ ì‹œë‚˜ë¦¬ì˜¤
1. **12/21 ì „ì²´ í•™ìŠµ** â†’ ì €ì¥
2. **12/25 ì ì§„ì  í•™ìŠµ** (12/22~12/24 ë°ì´í„° ì¶”ê°€)
   - ì˜ˆì¸¡ ì •í™•ë„ ë¹„êµ (ì „ì²´ ì¬í•™ìŠµ vs ì ì§„ì )
   - í•™ìŠµ ì‹œê°„ ì¸¡ì •
3. **ì—ì§€ ì¼€ì´ìŠ¤**:
   - ì‹ ê·œ ë°ì´í„° 1ì¼ì¹˜ë§Œ ìˆì„ ë•Œ
   - ë©”íƒ€ë°ì´í„° ì—†ëŠ” êµ¬ ëª¨ë¸ í˜¸í™˜ì„±

---

## 5. ê¸°ëŒ€ íš¨ê³¼

| í•­ëª© | ì „ì²´ ì¬í•™ìŠµ | ì ì§„ì  í•™ìŠµ |
|---|---|---|
| **í•™ìŠµ ì‹œê°„** (1ë…„ ë°ì´í„°) | ~3ë¶„ | ~30ì´ˆ âš¡ |
| **ë°ì´í„° ì‚¬ìš©ëŸ‰** | 252ì¼ì¹˜ ì „ì²´ | 4ì¼ì¹˜ ì‹ ê·œ + 25ì¼ì¹˜ Replay |
| **ì„±ëŠ¥ ìœ ì§€** | âœ… | âœ… (Replay Bufferë¡œ ë³´ì¥) |

---

## 6. ë¦¬ìŠ¤í¬ ê´€ë¦¬

| ë¦¬ìŠ¤í¬ | ì™„í™” ë°©ì•ˆ |
|---|---|
| Catastrophic Forgetting | Replay Buffer 10% + ë‚®ì€ LR |
| ì‹ ê·œ ë°ì´í„° ë„ˆë¬´ ì ìŒ (1~2ì¼) | ìµœì†Œ 3ì¼ ì´ìƒì¼ ë•Œë§Œ ì ì§„ì  í•™ìŠµ ê¶Œì¥ UI í‘œì‹œ |
| ë©”íƒ€ë°ì´í„° ì†ì‹¤ | í˜¸í™˜ì„± ë ˆì´ì–´: ë©”íƒ€ë°ì´í„° ì—†ìœ¼ë©´ ì „ì²´ í•™ìŠµ |

---

## 7. êµ¬í˜„ ìš°ì„ ìˆœìœ„

- [P0] Task 1, 2: ë©”íƒ€ë°ì´í„° ì¸í”„ë¼ (ë‹¤ë¥¸ ê¸°ëŠ¥ í˜¸í™˜ì„± ì˜í–¥)
- [P0] Task 3: LSTM Fine-tuning (í•µì‹¬ ëª¨ë¸)
- [P1] Task 4: XGBoost Incremental
- [P1] Task 5, 6: UI/UX
- [P1] Task 7: Ensemble í†µí•©
- [P2] ê²€ì¦ ë° ìµœì í™”
